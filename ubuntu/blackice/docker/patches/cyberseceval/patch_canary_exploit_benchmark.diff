--- PurpleLlama/CybersecurityBenchmarks/benchmark/canary_exploit_benchmark.py
+++ PurpleLlama/CybersecurityBenchmarks/benchmark/canary_exploit_benchmark.py
@@ -8,6 +8,7 @@
 import json
 import logging
 from pathlib import Path
+from typing import Optional
 
 from ..datasets.canary_exploit.verify_response import verify_responses
 
@@ -26,13 +27,26 @@
         return ["canary-exploit"]
 
     def query_llm_to_generate_responses(
-        self, prompt_path: Path, run_llm_in_parallel: int = 1
+        self, prompt_path: Path, run_llm_in_parallel: int = 1, system_prompt: Optional[str] = None,
     ) -> None:
         """
         Processes a dataset of prompts by sending them to the LLM and saving the responses in a file.
         """
         response_result = []
-        guided_decode_json_schema = '{"type": "object", "properties": {"answer": {"type": "string"}}, "required": ["answer"]}'
+        guided_decode_json_schema = {
+            "type": "json_schema",
+            "json_schema": {
+                "name": "canary_exploit_benchmark",
+                "schema": {
+                    "type": "object",
+                    "properties": {
+                        "answer": {"type": "string"}
+                    },
+                    "required": ["answer"]
+                },
+                "strict": True
+            }
+        }
         for llm in self.llms_under_test:
             logging.info(f"Querying {llm.model}")
             response_result.extend(
@@ -43,6 +57,7 @@
                     num_test_cases=self.num_test_cases,
                     pass_k=self.pass_k,
                     guided_decode_json_schema=guided_decode_json_schema,
+                    system_prompt=system_prompt,
                 )
             )
 
