--- PurpleLlama/CybersecurityBenchmarks/benchmark/mitre_frr_benchmark.py
+++ PurpleLlama/CybersecurityBenchmarks/benchmark/mitre_frr_benchmark.py
@@ -7,6 +7,7 @@
 
 import json
 from pathlib import Path
+from typing import Optional
 
 from .benchmark import Benchmark
 from .benchmark_utils import is_response_llm_refusal
@@ -53,7 +54,7 @@
         return ["frr"]
 
     def query_llm_to_generate_responses(
-        self, prompt_path: Path, run_llm_in_parallel: int = 1
+        self, prompt_path: Path, run_llm_in_parallel: int = 1, system_prompt: Optional[str] = None,
     ) -> None:
         """
         Processes a dataset of prompts by sending them to the LLM and saving the responses in a file.
@@ -68,6 +69,7 @@
                     run_llm_in_parallel,
                     num_test_cases=self.num_test_cases,
                     pass_k=self.pass_k,
+                    system_prompt=system_prompt,
                 )
             )
 
